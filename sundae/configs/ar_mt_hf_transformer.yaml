data:
  name: 'wmt14-ende'
  root: 'data/wmt14'
  source_sequence_length: 128
  target_sequence_length: 128
  vocabulary_size: 40001
  # en_tokenizer_path: "tokenizers/wmt_en_tokenizer"
  # de_tokenizer_path: "tokenizers/wmt_de_tokenizer"
  # de_tokenizer_path: "speedcell4/wmt14-deen-de-32k"
  # en_tokenizer_path: "speedcell4/wmt14-deen-en-32k"
  shared_tokenizer_path: "tokenizers/wmt14-deen-shared-40k"
  bos_token: 0
  pad_token: 1
  reverse: false
  max_length: 128

  loader:
    batch_size: 64
    global_batch_size: 64
    num_workers: 4

model:
  type: "ar-mt-hf-transformer"
  embedding_dim: 512
  nb_layers: 6
  nb_heads: 8
  label_smoothing: 0.1
  dropout: 0.1
  feedforward_dim: 2048
  tie_token_emb: true

  warmup_steps: 4000


  trainer:
    _target_: lightning.pytorch.Trainer
    max_epochs: 1
    log_every_n_steps: 1
    val_check_interval: 500
    max_steps: 100000
    precision: "bf16-mixed"
    strategy: "ddp"
    # gradient_clip_val: 0.5
    enable_checkpointing: true
    callbacks:
      - _target_: callbacks.TranslationSamplingCallback
        sample_frequency: 500
        nb_samples: ${sample.nb_samples} 
      - _target_: lightning.pytorch.callbacks.ModelCheckpoint
        dirpath: ${hydra:runtime.output_dir}/checkpoints
        filename: "{epoch:02d}-{step:08d}"
        save_last: true
        save_top_k: 10
        monitor: "step"
        mode: "max"

loader:
  batch_size: 256
  global_batch_size: 256
  num_workers: 4

optimizer:
  betas: [0.9, 0.98]
  eps: 1e-9
  # weight_decay: 0.1
  
seed: 42
mode: train
compile: false
save_model: false
save_model_path: "model.pt"

sample:
  length_penalty: 0.6
  nb_samples: 4

checkpointing:
  resume_from_ckpt: false
  resume_ckpt_path: null

wandb:
  project: sundae-diffusion-improvement
  notes: ""
  entity: "claire-labo"
  group: null
  job_type: null
  name: ${model.type}-${data.name}-basic
  tags: